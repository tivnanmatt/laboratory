Directory: .
./medmnist_environment.yml
Contents of ./medmnist_environment.yml:

name: medmnist
channels:
  - defaults
  - conda-forge
dependencies:
  - python=3.8
  - pip
  - pip:
    - medmnist
    - torch
    - torchvision
    - numpy
    - pandas
    - scikit-learn
    - pillow

----------------------------------------

./.gitignore
./out.txt
./sample.py
Contents of ./sample.py:

import torch
import matplotlib.pyplot as plt
from common import get_diffusion_bridge_model, load_weights
import laboratory as lab

# Get the model
diffusion_bridge_model = get_diffusion_bridge_model(train=False)

# Load pre-trained weights
diffusion_backbone_weights_filename = 'weights/diffusion_backbone_weights.pth'
load_weights(diffusion_bridge_model, diffusion_backbone_weights_filename)

# Set the model to evaluation mode
diffusion_bridge_model.eval()

# Generate samples
num_images = 16
num_measurements = 4
num_reconstructions = 4
true_images = torch.zeros(num_images, 1, 1, 1, 28, 28)
measurements = torch.zeros(num_images, num_measurements, 1, 1, 28, 28)
reconstructions = torch.zeros(num_images, num_measurements, num_reconstructions, 1, 28, 28)

for i in range(num_images):
    true_image, _ = diffusion_bridge_model.image_dataset[i:i+1]
    true_images[i] = true_image.reshape(1, 1, 1, 28, 28)


    for j in range(num_measurements):
        measurement = diffusion_bridge_model.measurement_simulator(true_image)
        measurements[i, j] = measurement
        for k in range(num_reconstructions):
            print(f"Generating sample {i+1}/{num_images}, measurement {j+1}/{num_measurements}, reconstruction {k+1}/{num_reconstructions}")
            with torch.no_grad():
                # reconstruction = diffusion_bridge_model.image_reconstructor(measurement)
                t = torch.ones(1, 1).to(measurement.device)
                assert isinstance(diffusion_bridge_model.image_reconstructor, lab.torch.tasks.reconstruction.DiffusionBridgeImageReconstructor)
                assert isinstance(diffusion_bridge_model.image_reconstructor.diffusion_model, lab.torch.diffusion.UnconditionalDiffusionModel)
                # reconstruction = diffusion_bridge_model.image_reconstructor.diffusion_model.diffusion_backbone(measurement, t)
                reconstruction = diffusion_bridge_model.image_reconstructor(measurement, num_timesteps=32)
            reconstructions[i, j, k] = reconstruction.reshape(1, 28, 28)
            
# Create animation
import matplotlib.animation as animation

fig, axs = plt.subplots(1, 3, figsize=(15, 5))
im0 = axs[0].imshow(true_images[0, 0, 0, 0], cmap='gray', vmin=-3, vmax=3)
axs[0].set_title('True Images')
im1 = axs[1].imshow(measurements[0, 0, 0, 0], cmap='gray', vmin=-3, vmax=3)
axs[1].set_title('Measurements')
im2 = axs[2].imshow(reconstructions[0, 0, 0, 0], cmap='gray', vmin=-3, vmax=3)
axs[2].set_title('Reconstructions')

def animate(i):
    print('Animating frame {}/{}'.format(i+1, num_images*num_measurements*num_reconstructions))
    i, j, k = i // (num_measurements*num_reconstructions), (i // num_reconstructions) % num_measurements, i % num_reconstructions
    im0.set_array(true_images[i, 0, 0, 0])
    im1.set_array(measurements[i, j, 0, 0])
    im2.set_array(reconstructions[i, j, k, 0])
    return im0, im1, im2
    

ani = animation.FuncAnimation(fig, animate, frames=num_images*num_measurements*num_reconstructions, interval=1000, repeat=False)

# mp4 writer ffmpeg
writer = animation.writers['ffmpeg'](fps=10)
ani.save('figures/diffusion_bridge_model.mp4', writer=writer)

plt.show()

----------------------------------------

./download_datasets.sh
Contents of ./download_datasets.sh:

#!/bin/bash

# Check if the conda environment already exists
if conda env list | grep -q 'medmnist'; then
  echo "The conda environment 'medmnist' already exists. Skipping environment creation."
else
  # Create the conda environment from the environment_medmnist.yml file
  conda env create --name medmnist --file medmnist_environment.yml
fi

# Initialize conda in the current shell session
eval "$(conda shell.bash hook)"

# Activate the newly created conda environment
conda activate medmnist

# Run the Python script to download the datasets
python download_datasets.py

# Deactivate the conda environment
conda deactivate

----------------------------------------

./download_datasets.py
Contents of ./download_datasets.py:

import medmnist
from medmnist import INFO
import numpy as np
import os

# Ensure the output directory exists
os.makedirs('medmnist_data', exist_ok=True)

def save_dataset(dataset, split):
    images, labels = dataset.imgs, dataset.labels
    np.save(f'medmnist_data/{dataset.flag}_{split}_images.npy', images)
    np.save(f'medmnist_data/{dataset.flag}_{split}_labels.npy', labels)

for data_flag in ['pathmnist', 'chestmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'retinamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist']:

    info = INFO[data_flag]
    DataClass = getattr(medmnist, info['python_class'])

    # Download and save train dataset
    train_dataset = DataClass(split='train', download=True, root='medmnist_data')
    save_dataset(train_dataset, 'train')

    # Download and save test dataset
    test_dataset = DataClass(split='test', download=True, root='medmnist_data')
    save_dataset(test_dataset, 'test')

    # Download and save validation dataset if available
    if info['task'] == 'multi-class':
        val_dataset = DataClass(split='val', download=True, root='medmnist_data')
        save_dataset(val_dataset, 'val')

print("Datasets downloaded and saved successfully.")

----------------------------------------

./common.py
Contents of ./common.py:

import os
import torch
import laboratory as lab

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define the components of the DiffusionBridgeModel
def get_diffusion_bridge_model(measurement_noise_variance=0.1, train=True):
    # Dataset
    image_dataset = lab.torch.datasets.Medmnist_OrganA(
                            root='medmnist_data',
                            train=True).to(device)
    
    # Measurement Noise Simulator
    measurement_simulator = lab.torch.distributions.gaussian.AdditiveWhiteGaussianNoise(
                                noise_variance=measurement_noise_variance).to(device)
    
    # Noise Variance Function
    def noise_variance_fn(t):
        if isinstance(t, int) or isinstance(t, float):
            t = torch.tensor(t, dtype=torch.float32)
        t = t.view(-1, 1, 1, 1)
        return t * measurement_noise_variance

    # Noise Variance Derivative Function
    def noise_variance_prime_fn(t):
        t = t.view(-1, 1, 1, 1)
        return (0 * t + 1) * measurement_noise_variance

    # Forward SDE
    forward_SDE = lab.torch.sde.SongVarianceExplodingProcess(
                        noise_variance=noise_variance_fn, 
                        noise_variance_prime=noise_variance_prime_fn)

    # Image Encoder
    image_encoder = lab.torch.linalg.IdentityLinearOperator().to(device)

    # Time Encoder
    time_encoder = lab.torch.networks.DenseNet(input_shape=(1,), 
                                               output_shape=(1, 28, 28), 
                                               hidden_channels_list=[782, 782], 
                                               activation='relu').to(device)

    # # Final Estimator
    # class FinalEstimator(torch.nn.Module):
    #     def __init__(self):
    #         super(FinalEstimator, self).__init__()
    #         self.densenet = lab.torch.networks.DenseNet(
    #                             input_shape=(2, 28, 28), 
    #                             output_shape=(1, 28, 28), 
    #                             hidden_channels_list=[782, 782, 782, 782], 
    #                             activation='relu')
    #         self.head = lab.torch.networks.DenseNet(
    #                         input_shape=(1, 28, 28), 
    #                         output_shape=(1, 28, 28), 
    #                         hidden_channels_list=[782], 
    #                         activation='linear')

    #     def forward(self, image_embedding, time_embedding):
    #         concatenated_data = torch.cat([image_embedding, time_embedding], dim=1)
    #         densenet_output = self.densenet(concatenated_data)
    #         return self.head(densenet_output)
    
    # final_estimator = FinalEstimator().to(device)




    # lets make a final estimator that does a convolutional u-net

    class Unet_FinalEstimator(torch.nn.Module):
        def __init__(self):
            super(Unet_FinalEstimator, self).__init__()
            # input shape is 2, 28, 28
            # output shape is 1, 28, 28

            class ConvBlock(torch.nn.Module):
                def __init__(self, in_channels, out_channels, activation='relu'):
                    super(ConvBlock, self).__init__()
                    self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
                    self.bn = torch.nn.BatchNorm2d(out_channels)
                    if activation == 'relu':
                        self.activation = torch.nn.ReLU()
                    elif activation == 'linear':
                        self.activation = torch.nn.Identity()
                def forward(self, x):
                    return self.activation(self.bn(self.conv(x)))

            class DownConvBlock(torch.nn.Module):
                def __init__(self, in_channels, out_channels, activation='relu'):
                    super(DownConvBlock, self).__init__()
                    self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=2)
                    self.bn = torch.nn.BatchNorm2d(out_channels)
                    if activation == 'relu':
                        self.activation = torch.nn.ReLU()
                    elif activation == 'linear':
                        self.activation = torch.nn.Identity()
                def forward(self, x):
                    return self.activation(self.bn(self.conv(x)))

            class UpConvBlock(torch.nn.Module):
                def __init__(self, in_channels, out_channels, activation='relu'):
                    super(UpConvBlock, self).__init__()
                    self.conv = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, padding=1, stride=2, output_padding=1)
                    self.bn = torch.nn.BatchNorm2d(out_channels)
                    if activation == 'relu':
                        self.activation = torch.nn.ReLU()
                    elif activation == 'linear':
                        self.activation = torch.nn.Identity()
                def forward(self, x):
                    return self.activation(self.bn(self.conv(x)))

            base_channels=64
            self.conv1 = ConvBlock(2, base_channels) # -> base_channels, 28, 28
            self.conv2 = ConvBlock(base_channels, base_channels) # -> base_channels, 28, 28
            self.down3 = DownConvBlock(base_channels, base_channels * 2) # -> base_channels * 2, 14, 14
            self.conv4 = ConvBlock(base_channels * 2, base_channels * 2) # -> base_channels * 2, 14, 14
            self.conv5 = ConvBlock(base_channels * 2, base_channels * 2) # -> base_channels * 2, 14, 14
            self.down6 = DownConvBlock(base_channels * 2, base_channels * 4) # -> base_channels * 4, 7, 7
            self.conv7 = ConvBlock(base_channels * 4, base_channels * 4) # -> base_channels * 4, 7, 7
            self.flatten = torch.nn.Flatten()
            self.fc1 = torch.nn.Linear(base_channels * 4 * 7 * 7, base_channels * 4 * 7 * 7)
            self.fc2 = torch.nn.Linear(base_channels * 4 * 7 * 7, base_channels * 4 * 7 * 7)
            self.conv8 = ConvBlock(base_channels * 4, base_channels * 4) # -> base_channels * 4, 7, 7
            self.up9 = UpConvBlock(base_channels * 4, base_channels * 2) # -> base_channels * 2, 14, 14
            self.conv10 = ConvBlock(base_channels * 4, base_channels * 2) # -> base_channels * 2, 14, 14
            self.conv11 = ConvBlock(base_channels * 2, base_channels * 2) # -> base_channels * 2, 14, 14
            self.up12 = UpConvBlock(base_channels * 2, base_channels) # -> base_channels, 28, 28
            self.conv13 = ConvBlock(base_channels * 2, base_channels) # -> base_channels, 28, 28
            self.conv14 = ConvBlock(base_channels, base_channels) # -> base_channels, 28, 28
            self.conv15 = ConvBlock(base_channels, 1, activation='linear') # -> 1, 28, 28


        def forward(self, image_embedding, time_embedding):
            concatenated_data = torch.cat([image_embedding, time_embedding], dim=1)
            x1 = self.conv1(concatenated_data)
            x2 = self.conv2(x1)
            x3 = self.down3(x2)
            x4 = self.conv4(x3)
            x5 = self.conv5(x4) 
            x6 = self.down6(x5)
            x7 = self.conv7(x6)
            x8 = self.conv8(x7)
            x9 = self.up9(x8)
            x10 = self.conv10(torch.cat([x9, x5], dim=1))
            x11 = self.conv11(x10)
            x12 = self.up12(x11)
            x13 = self.conv13(torch.cat([x12, x2], dim=1))
            x14 = self.conv14(x13)
            x15 = self.conv15(x14)
            return x15
        
    final_estimator = Unet_FinalEstimator().to(device)

    # Diffusion Backbone
    diffusion_backbone = lab.torch.diffusion.UnconditionalDiffusionBackbone(
                            image_encoder=image_encoder,
                            time_encoder=time_encoder,
                            final_estimator=final_estimator).to(device)

    # Diffusion Model
    diffusion_model = lab.torch.diffusion.UnconditionalDiffusionModel(
                            forward_SDE=forward_SDE,
                            diffusion_backbone=diffusion_backbone,
                            estimator_type='mean').to(device)
    

    # Initial and Final Reconstructor
    initial_reconstructor = lab.torch.linalg.IdentityLinearOperator()
    final_reconstructor = lab.torch.linalg.IdentityLinearOperator()

    # Diffusion Bridge Image Reconstructor
    image_reconstructor = lab.torch.tasks.reconstruction.DiffusionBridgeImageReconstructor(
                            initial_reconstructor=initial_reconstructor,
                            diffusion_model=diffusion_model,
                            final_reconstructor=final_reconstructor
                            ).to(device)

    # Diffusion Bridge Model
    diffusion_bridge_model = lab.torch.tasks.reconstruction.DiffusionBridgeModel(
                                image_dataset=image_dataset,
                                measurement_simulator=measurement_simulator,
                                image_reconstructor=image_reconstructor,
                                task_evaluator='rmse'
                                ).to(device)

    return diffusion_bridge_model

def load_weights(diffusion_bridge_model, filename):
    if torch.cuda.is_available() and os.path.exists(filename):
        diffusion_bridge_model.image_reconstructor.diffusion_model.diffusion_backbone.load_state_dict(torch.load(filename))
    elif os.path.exists(filename):
        diffusion_bridge_model.image_reconstructor.diffusion_model.diffusion_backbone.load_state_dict(torch.load(filename, map_location=torch.device('cpu')))
    else:
        print(f"Weights file '{filename}' not found.")

def save_weights(diffusion_bridge_model, filename):
    torch.save(diffusion_bridge_model.image_reconstructor.diffusion_model.diffusion_backbone.state_dict(), filename)

----------------------------------------

./train.py
Contents of ./train.py:

import os
import torch
from common import get_diffusion_bridge_model, load_weights, save_weights

# Get the model
diffusion_bridge_model = get_diffusion_bridge_model()

# Load pre-trained weights if available
diffusion_backbone_weights_filename = 'weights/diffusion_backbone_weights.pth'
load_weights(diffusion_bridge_model, diffusion_backbone_weights_filename)

for i in range(100):
    # Train the model
    diffusion_bridge_model.train_diffusion_backbone(batch_size=32, num_epochs=10, num_iterations=100, verbose=True)

    # Save the weights after training
    save_weights(diffusion_bridge_model, diffusion_backbone_weights_filename)

----------------------------------------

./print_code.py
Contents of ./print_code.py:

import os

def print_file_contents(file_path):
    with open(file_path, 'r') as file:
        print(f"Contents of {file_path}:\n")
        print(file.read())
        print("\n" + "-"*40 + "\n")

def list_files_and_print_contents(root_dir='.'):
    for dirpath, dirnames, filenames in os.walk(root_dir):
        print(f"Directory: {dirpath}")
        for filename in filenames:
            file_path = os.path.join(dirpath, filename)
            print(file_path)
            if filename.endswith(('.py', '.sh', '.yml')):
                print_file_contents(file_path)

if __name__ == "__main__":
    list_files_and_print_contents()

----------------------------------------

Directory: ./figures
./figures/.gitignore
Directory: ./weights
./weights/.gitignore
